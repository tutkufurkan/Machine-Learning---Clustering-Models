{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dandrandandran2093/machine-learning-clustering-models?scriptVersionId=278474949\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# üîç INSTRUCTION\n1. [K-Means Clustering](#1)\n2. [Hierarchical Clustering](#2)","metadata":{}},{"cell_type":"markdown","source":"# üìä Data Exploration\n\n## What is Clustering?\nClustering is an **unsupervised learning** technique that groups similar data points together without predefined labels. It discovers patterns and structures in data.\n\n## Supervised vs Unsupervised Learning:\n- **Supervised**: Has labels (y) - Classification & Regression\n- **Unsupervised**: No labels - Clustering & Dimensionality Reduction","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\n\n# machine learning\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:07:48.363529Z","iopub.execute_input":"2025-11-07T20:07:48.364266Z","iopub.status.idle":"2025-11-07T20:07:53.241775Z","shell.execute_reply.started":"2025-11-07T20:07:48.364233Z","shell.execute_reply":"2025-11-07T20:07:53.240372Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# K-Means Clustering","metadata":{}},{"cell_type":"markdown","source":"### What is K-Means Clustering?\nAn **unsupervised learning algorithm** that partitions data into **K distinct clusters** based on feature similarity.\n**\"Means\"** refers to calculating the average (centroid) of data points in each cluster!\n\n---\n\n### Algorithm Steps\n$$\\text{Minimize: } \\sum_{i=1}^{K}\\sum_{x \\in C_i}||x - \\mu_i||^2$$\n\n**Simple terms:**\n1. Choose number of clusters (K)\n2. Randomly initialize K centroids\n3. Assign each point to nearest centroid\n4. Update centroids (mean of assigned points)\n5. Repeat steps 3-4 until convergence\n\n---\n\n### Elbow Method\nUsed to find optimal K value by plotting:\n- **X-axis:** Number of clusters (K)\n- **Y-axis:** Within-Cluster Sum of Squares (WCSS)\n- **Elbow Point:** Where WCSS decrease rate slows down\n\n---\n\n### Key Parameters\n- **n_clusters:** Number of clusters (K)\n- **init:** Centroid initialization method ('k-means++' recommended)\n- **max_iter:** Maximum iterations (default: 300)\n- **random_state:** For reproducibility\n\n---\n\n### Pros & Cons\n**Advantages:**\n- Fast and efficient\n- Works well with large datasets\n- Simple to implement and interpret\n- Scales well to high dimensions\n\n**Disadvantages:**\n- Need to specify K beforehand\n- Sensitive to initial centroid positions\n- Assumes spherical clusters\n- Affected by outliers","metadata":{}},{"cell_type":"markdown","source":"## Create Synthetic Dataset","metadata":{}},{"cell_type":"code","source":"# Create Dataset\n# Class 1 (Bottom-left cluster)\nx1 = np.random.normal(25, 5, 1000)  # mean=25, std=5, 1000 points\ny1 = np.random.normal(25, 5, 1000)\n\n# Class 2 (Top-right cluster)\nx2 = np.random.normal(55, 5, 1000)\ny2 = np.random.normal(60, 5, 1000)\n\n# Class 3 (Bottom-right cluster)\nx3 = np.random.normal(55, 5, 1000)\ny3 = np.random.normal(15, 5, 1000)\n\n# Combine all clusters\nx = np.concatenate((x1, x2, x3), axis=0)\ny = np.concatenate((y1, y2, y3), axis=0)\n\n# Create DataFrame\ndictionary = {\"x\": x, \"y\": y}\ndata = pd.DataFrame(dictionary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:07:53.244244Z","iopub.execute_input":"2025-11-07T20:07:53.245115Z","iopub.status.idle":"2025-11-07T20:07:53.259227Z","shell.execute_reply.started":"2025-11-07T20:07:53.245079Z","shell.execute_reply":"2025-11-07T20:07:53.257977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:07:53.260166Z","iopub.execute_input":"2025-11-07T20:07:53.260509Z","iopub.status.idle":"2025-11-07T20:07:53.315804Z","shell.execute_reply.started":"2025-11-07T20:07:53.260482Z","shell.execute_reply":"2025-11-07T20:07:53.314419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:07:53.318261Z","iopub.execute_input":"2025-11-07T20:07:53.318581Z","iopub.status.idle":"2025-11-07T20:07:53.344167Z","shell.execute_reply.started":"2025-11-07T20:07:53.318557Z","shell.execute_reply":"2025-11-07T20:07:53.343125Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize Ground Truth (Separated Clusters)","metadata":{}},{"cell_type":"code","source":"# Plot Data with True Labels\nplt.figure(figsize=(10, 6))\nplt.scatter(x1, y1, alpha=0.6, label='Cluster 1', s=50)\nplt.scatter(x2, y2, alpha=0.6, label='Cluster 2', s=50)\nplt.scatter(x3, y3, alpha=0.6, label='Cluster 3', s=50)\nplt.xlabel(\"X values\", fontsize=12)\nplt.ylabel(\"Y values\", fontsize=12)\nplt.title(\"Ground Truth: 3 Clusters (Separated)\", fontsize=14)\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:10:35.315204Z","iopub.execute_input":"2025-11-07T20:10:35.31554Z","iopub.status.idle":"2025-11-07T20:10:35.644431Z","shell.execute_reply.started":"2025-11-07T20:10:35.315515Z","shell.execute_reply":"2025-11-07T20:10:35.643324Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualize Combined Data","metadata":{}},{"cell_type":"code","source":"# Plot Actual Data (Without Labels - What K-Means Sees)\nplt.figure(figsize=(10, 6))\nplt.scatter(x, y, alpha=0.5, s=30, c='gray')\nplt.xlabel(\"X values\", fontsize=12)\nplt.ylabel(\"Y values\", fontsize=12)\nplt.title(\"Data Without Labels (Unsupervised Challenge)\", fontsize=14)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:10:50.455279Z","iopub.execute_input":"2025-11-07T20:10:50.455568Z","iopub.status.idle":"2025-11-07T20:10:50.71669Z","shell.execute_reply.started":"2025-11-07T20:10:50.455548Z","shell.execute_reply":"2025-11-07T20:10:50.715699Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Elbow Method - Finding Optimal K","metadata":{}},{"cell_type":"code","source":"# KMeans - Calculate WCSS for different K values\nwcss = []\n\nfor k in range(1, 15):\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_)\n\n# Plot Elbow Curve\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 15), wcss, marker='o', linestyle='--', linewidth=2, markersize=8)\nplt.xlabel(\"Number of Clusters (K)\", fontsize=12)\nplt.ylabel(\"WCSS (Within-Cluster Sum of Squares)\", fontsize=12)\nplt.title(\"Elbow Method: Finding Optimal K\", fontsize=14)\nplt.xticks(range(1, 15))\nplt.grid(True, alpha=0.3)\nplt.axvline(x=3, color='r', linestyle='--', label='Optimal K=3', alpha=0.7)\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:07:54.211239Z","iopub.execute_input":"2025-11-07T20:07:54.211586Z","iopub.status.idle":"2025-11-07T20:07:55.360927Z","shell.execute_reply.started":"2025-11-07T20:07:54.211551Z","shell.execute_reply":"2025-11-07T20:07:55.35981Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## K-Means with K=3","metadata":{}},{"cell_type":"code","source":"# K equals 3 model\nkmeans2 = KMeans(n_clusters=3, random_state=42)\nclusters = kmeans2.fit_predict(data)\n\n# Add cluster labels\ndata[\"label\"] = clusters\n\nprint(\"First 5 samples with cluster labels:\")\nprint(data.head())\nprint(\"-\" * 50)\nprint(\"Last 5 samples with cluster labels:\")\nprint(data.tail())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:07:55.361829Z","iopub.execute_input":"2025-11-07T20:07:55.362104Z","iopub.status.idle":"2025-11-07T20:07:55.402169Z","shell.execute_reply.started":"2025-11-07T20:07:55.362081Z","shell.execute_reply":"2025-11-07T20:07:55.400902Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## K-Means Results Visualization","metadata":{}},{"cell_type":"code","source":"# Plot K-Means Results\nplt.figure(figsize=(12, 8))\n\nplt.scatter(data.x[data.label == 0], data.y[data.label == 0], \n           color=\"red\", label=\"Cluster 0\", alpha=0.6, s=50)\nplt.scatter(data.x[data.label == 1], data.y[data.label == 1], \n           color=\"green\", label=\"Cluster 1\", alpha=0.6, s=50)\nplt.scatter(data.x[data.label == 2], data.y[data.label == 2], \n           color=\"blue\", label=\"Cluster 2\", alpha=0.6, s=50)\n\n# Plot centroids\nplt.scatter(kmeans2.cluster_centers_[:, 0], \n           kmeans2.cluster_centers_[:, 1], \n           color=\"yellow\", marker='*', s=500, \n           edgecolors='black', linewidths=2,\n           label=\"Centroids\", zorder=5)\n\nplt.xlabel(\"X values\", fontsize=12)\nplt.ylabel(\"Y values\", fontsize=12)\nplt.title(\"K-Means Clustering Results (K=3)\", fontsize=14)\nplt.legend(fontsize=10)\nplt.show()\n\nprint(f\"\\nCluster Centers:\\n{kmeans2.cluster_centers_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:07:55.402907Z","iopub.execute_input":"2025-11-07T20:07:55.403164Z","iopub.status.idle":"2025-11-07T20:07:55.819361Z","shell.execute_reply.started":"2025-11-07T20:07:55.403143Z","shell.execute_reply":"2025-11-07T20:07:55.818233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# Hierarchical Clustering","metadata":{}},{"cell_type":"markdown","source":"### What is Hierarchical Clustering?\nA **clustering method** that builds a **hierarchy of clusters** without specifying K beforehand.\n**\"Hierarchical\"** because it creates a tree-like structure (dendrogram) showing relationships!\n\n---\n\n### Types & Algorithm\n$$\\text{Distance: } d(C_i, C_j) = \\min_{x \\in C_i, y \\in C_j} ||x - y||$$\n\n**Two Approaches:**\n- **Agglomerative (Bottom-up):** Start with individual points, merge similar ones\n- **Divisive (Top-down):** Start with one cluster, split recursively\n\n**Linkage Methods:**\n- **Single:** Minimum distance between clusters\n- **Complete:** Maximum distance between clusters\n- **Average:** Average distance between all points\n- **Ward:** Minimizes within-cluster variance *(most common)*\n\n---\n\n### Dendrogram\nVisual representation of hierarchical clustering:\n- **Y-axis:** Distance/dissimilarity\n- **X-axis:** Data points\n- **Horizontal line:** Cutting line to determine number of clusters\n\n---\n\n### Key Parameters\n- **n_clusters:** Number of clusters (can be determined from dendrogram)\n- **linkage:** Method to calculate distance ('ward', 'complete', 'average')\n- **affinity:** Distance metric ('euclidean', 'manhattan', 'cosine')\n\n---\n\n### Pros & Cons\n**Advantages:**\n- No need to specify K beforehand\n- Produces dendrogram for visualization\n- Works with any distance metric\n- Captures hierarchical relationships\n\n**Disadvantages:**\n- Computationally expensive (O(n¬≥))\n- Not suitable for large datasets\n- Sensitive to noise and outliers\n- Once merged, cannot undo (agglomerative)","metadata":{}},{"cell_type":"markdown","source":"## Create Smaller Dataset (For Performance)\n\n**Note:** Hierarchical clustering is computationally expensive, so we use a smaller dataset (100 points per cluster instead of 1000)","metadata":{}},{"cell_type":"code","source":"# Create Dataset (smaller for hierarchical - 100 points per cluster)\n# Class 1\nx1_ = np.random.normal(25, 5, 100)\ny1_ = np.random.normal(25, 5, 100)\n\n# Class 2\nx2_ = np.random.normal(55, 5, 100)\ny2_ = np.random.normal(60, 5, 100)\n\n# Class 3\nx3_ = np.random.normal(55, 5, 100)\ny3_ = np.random.normal(15, 5, 100)\n\nx_ = np.concatenate((x1_, x2_, x3_), axis=0)\ny_ = np.concatenate((y1_, y2_, y3_), axis=0)\n\ndictionary_ = {\"x\": x_, \"y\": y_}\ndata_ = pd.DataFrame(dictionary_)\n\nprint(f\"Dataset size for Hierarchical Clustering: {len(data_)} points\")\nprint(f\"(Smaller than K-Means for computational efficiency)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:07:55.821549Z","iopub.execute_input":"2025-11-07T20:07:55.821917Z","iopub.status.idle":"2025-11-07T20:07:55.831452Z","shell.execute_reply.started":"2025-11-07T20:07:55.821894Z","shell.execute_reply":"2025-11-07T20:07:55.83016Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dendrogram Visualization","metadata":{}},{"cell_type":"code","source":"# Create linkage matrix\nmerg = linkage(data_, method=\"ward\")\n\n# Plot Dendrogram\nplt.figure(figsize=(14, 7))\ndendrogram(merg, leaf_rotation=90)\nplt.xlabel(\"Data Points\", fontsize=12)\nplt.ylabel(\"Euclidean Distance (Ward)\", fontsize=12)\nplt.title(\"Hierarchical Clustering Dendrogram\", fontsize=14)\nplt.axhline(y=100, color='r', linestyle='--', label='Cut line (3 clusters)', alpha=0.7)\nplt.legend()\nplt.grid(True, alpha=0.3, axis='y')\nplt.show()\n\nprint(\"\\nüí° Insight: The dendrogram clearly shows 3 distinct clusters!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:07:55.832862Z","iopub.execute_input":"2025-11-07T20:07:55.83325Z","iopub.status.idle":"2025-11-07T20:07:58.453583Z","shell.execute_reply.started":"2025-11-07T20:07:55.833224Z","shell.execute_reply":"2025-11-07T20:07:58.452666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hierarchical Clustering with K=3","metadata":{}},{"cell_type":"code","source":"# Hierarchical Clustering\nhierarchical_cluster = AgglomerativeClustering(n_clusters=3, affinity=\"euclidean\", linkage=\"ward\")\ncluster = hierarchical_cluster.fit_predict(data_)\n\n# Add cluster labels\ndata_[\"label\"] = cluster\n\nprint(f\"\\nCluster Distribution:\")\nprint(data_['label'].value_counts().sort_index())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:07:58.45468Z","iopub.execute_input":"2025-11-07T20:07:58.45493Z","iopub.status.idle":"2025-11-07T20:07:58.475752Z","shell.execute_reply.started":"2025-11-07T20:07:58.454909Z","shell.execute_reply":"2025-11-07T20:07:58.474581Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hierarchical Clustering Results","metadata":{}},{"cell_type":"code","source":"# Plot Hierarchical Clustering Results\nplt.figure(figsize=(12, 8))\n\nplt.scatter(data_.x[data_.label == 0], data_.y[data_.label == 0], \n           color=\"red\", label=\"Cluster 0\", alpha=0.6, s=50)\nplt.scatter(data_.x[data_.label == 1], data_.y[data_.label == 1], \n           color=\"green\", label=\"Cluster 1\", alpha=0.6, s=50)\nplt.scatter(data_.x[data_.label == 2], data_.y[data_.label == 2], \n           color=\"blue\", label=\"Cluster 2\", alpha=0.6, s=50)\n\nplt.xlabel(\"X values\", fontsize=12)\nplt.ylabel(\"Y values\", fontsize=12)\nplt.title(\"Hierarchical Clustering Results (K=3, Ward Linkage)\", fontsize=14)\nplt.legend(fontsize=10)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T20:07:58.47684Z","iopub.execute_input":"2025-11-07T20:07:58.477139Z","iopub.status.idle":"2025-11-07T20:07:58.815511Z","shell.execute_reply.started":"2025-11-07T20:07:58.477115Z","shell.execute_reply":"2025-11-07T20:07:58.814524Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üéØ Key Takeaways\n\n## Model Comparison\n\n| Feature | K-Means | Hierarchical |\n|---------|---------|--------------|  \n| **Speed** | Fast ‚ö° (1000 points/cluster) | Slow üê¢ (100 points/cluster) |\n| **K Selection** | Must specify | From dendrogram |\n| **Dataset Size** | Large ‚úÖ | Small/Medium üìè |\n| **Interpretability** | Good (centroids) | Excellent (dendrogram) |\n| **Cluster Shape** | Spherical | Any shape |\n| **Complexity** | O(n√óK√óiterations) | O(n¬≥) |\n\n## Important Insights\n\n- **K-Means** successfully identified 3 clusters in large dataset (3000 points)\n- **Elbow Method** correctly suggested K=3\n- **Hierarchical Clustering** also found 3 clusters using dendrogram\n- **Dendrogram** provides visual hierarchy - no need to guess K!\n- Both methods work excellently with well-separated, spherical clusters\n\n## When to Use Which?\n\n**Use K-Means when:**\n- Working with large datasets (10,000+ points)\n- Computational efficiency is important\n- Clusters are roughly spherical\n- You have approximate idea of K\n\n**Use Hierarchical when:**\n- Need to understand cluster hierarchy\n- Dataset is small/medium sized (< 5000 points)\n- Want to explore different K values\n- Interested in relationships between clusters\n\n## Performance Notes\n\n- **K-Means:** Processed 3,000 points efficiently\n- **Hierarchical:** Used 300 points (computational constraint)\n- For production: K-Means for large-scale, Hierarchical for analysis","metadata":{}},{"cell_type":"markdown","source":"# üîó References\n\n## üìö My Machine Learning Series\n\nThis notebook is part of a comprehensive Machine Learning series:\n\n| Notebook | Topics Covered |\n|----------|---------------|\n| üîç **Clustering Models** | K-Means, Hierarchical Clustering *(Current)* |\n| üî¨ **Advanced Topics** | [Link](https://www.kaggle.com/code/dandrandandran2093/machine-learning-advanced-topics) - NLP, PCA, Model Selection, Recommendations |\n| üéØ **Classification Models** | [Link](https://www.kaggle.com/code/dandrandandran2093/machine-learning-classifications-models) - Logistic Regression, KNN, SVM, etc. |\n| üìà **Regression Models** | [Link](https://www.kaggle.com/code/dandrandandran2093/machine-learning-regression-models) - Linear, Polynomial, Decision Tree, Random Forest |\n\n---\n\n**Course:** Udemy - MACHINE LEARNING by DATAI TEAM\n\n**Libraries:** NumPy, Pandas, Matplotlib, Plotly, Scikit-learn, SciPy","metadata":{}}]}